{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2935d9e1-663d-443b-af00-b5e3f28b9cc6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "This notebook logs a HuggingFace model with an input example and a model signature and registers it to the Databricks Model Registry.\n",
    "\n",
    "After you run this notebook in its entirety, you have a registered model for model serving with Databricks Model Serving ([AWS](https://docs.databricks.com/machine-learning/model-serving/index.html)|[Azure](https://learn.microsoft.com/azure/databricks/machine-learning/model-serving/index.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f12d3da8-bfc1-48c2-b182-fcbdb656a107",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "import mlflow\n",
    "import torch\n",
    "model_volume_location = \"/Volumes/adrian_test/genai/huggingface/bertweet_base\"\n",
    "tokenizer = transformers.BertweetTokenizer.from_pretrained(model_volume_location, normalization=True, cache_dir='tokenizer_cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41f92261-c605-43c5-ba73-703f33f32d1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class AugmentedBert(torch.nn.Module):\n",
    "    def __init__(self, output_class_len, base_model, cache_dir, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.bert_model = transformers.AutoModel.from_pretrained(base_model, cache_dir=cache_dir)\n",
    "        self.emb_dim = 768\n",
    "        self.fc1 = torch.nn.Linear(self.emb_dim, self.emb_dim)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "        self.gelu = torch.nn.GELU()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        bert_output = self.bert_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "        )\n",
    "        output = bert_output[\"last_hidden_state\"][:, 0, :]\n",
    "        output = self.fc1(output)\n",
    "        output = self.tanh(output)\n",
    "        output = self.gelu(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2211f0d4-87b3-470a-9ce7-bf46bb3162ce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model = AugmentedBert(10, model_volume_location, \"model_cache\")\n",
    "with mlflow.start_run():\n",
    "  mlflow.pytorch.log_model(model, 'pytorch-model', registered_model_name='bert-encoder-pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e2ab375-8898-4142-8390-63d2c5d419ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "class SampleDatasetWithEncodings(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            key: torch.tensor(val[idx]).clone().detach()\n",
    "            for key, val in self.encodings.items()\n",
    "        }\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "\n",
    "def create_data_loader(tokenizer, X, y=None, batch_size=1, input_max_len=64):\n",
    "    features = tokenizer(\n",
    "        X,\n",
    "        max_length=input_max_len,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    if y is not None:\n",
    "        dataset = SampleDatasetWithEncodings(features, y)\n",
    "    else:\n",
    "        dataset = SampleDatasetWithEncodings(features, [0] * features.get(\"input_ids\").shape[0])\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6a436d5c-c55c-4f09-bdf2-3d0789a20654",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository\n",
    "import os\n",
    "model_name = 'bert-encoder-pytorch'\n",
    "model_uri = f\"models:/{model_name}/1\"\n",
    "if not os.path.exists('/databricks/driver/pytorch-model-artifacts'):\n",
    "  os.makedirs('/databricks/driver/pytorch-model-artifacts')\n",
    "local_path = ModelsArtifactRepository(model_uri).download_artifacts(\"\", dst_path=\"/databricks/driver/pytorch-model-artifacts\") # download model from remote registry\n",
    "print(local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b202596-94d2-4a2d-87aa-1a065e3aa254",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import torch\n",
    "import pandas as pd\n",
    "import transformers\n",
    "\n",
    "class ModelPyfunc(mlflow.pyfunc.PythonModel):\n",
    "    def load_context(self, context):\n",
    "        self.model = torch.load(context.artifacts[\"torch-weights\"])\n",
    "        self.tokenizer = transformers.BertweetTokenizer.from_pretrained(model_volume_location, normalization=True, local_files_only=True, cache_dir=context.artifacts[\"tokenizer_cache\"])\n",
    "\n",
    "    def format_inputs(self, model_input):\n",
    "        if isinstance(model_input, str):\n",
    "            model_input = [model_input]\n",
    "        if isinstance(model_input, pd.Series):\n",
    "            model_input = model_input.tolist()\n",
    "        if isinstance(model_input, pd.DataFrame):\n",
    "            model_input = model_input.iloc[:, 0].tolist()\n",
    "        return model_input\n",
    "\n",
    "    def prepare_data(self, tokenizer, model_input):\n",
    "        data_loader = create_data_loader(\n",
    "            tokenizer,\n",
    "            model_input\n",
    "        )\n",
    "        return data_loader.dataset.encodings\n",
    "\n",
    "    def format_outputs(self, outputs):\n",
    "        predictions = (torch.sigmoid(outputs)).data.numpy()\n",
    "        classes = [\n",
    "            \"class1\",\n",
    "            \"class2\",\n",
    "            \"class3\",\n",
    "            \"class4\",\n",
    "            \"class5\",\n",
    "            \"class6\",\n",
    "        ]\n",
    "        return [dict(zip(classes, prediction)) for prediction in predictions]\n",
    "\n",
    "    def predict(self, context, model_input):\n",
    "        model_input = self.format_inputs(model_input)\n",
    "        processed_input = self.prepare_data(self.tokenizer, model_input)\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.forward(\n",
    "                input_ids=processed_input.get(\"input_ids\"),\n",
    "                attention_mask=processed_input.get(\"attention_mask\"),\n",
    "            )\n",
    "        return self.format_outputs(outputs)\n",
    "\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    model = ModelPyfunc()\n",
    "    mlflow.pyfunc.log_model(\n",
    "      \"model\",\n",
    "      python_model=model,\n",
    "      artifacts={'torch-weights': \"./pytorch-model-artifacts/data/model.pth\", \"tokenizer_cache\": \"./tokenizer_cache\"},\n",
    "      input_example=[\"this is a test\", \"this is a second test\"],\n",
    "      registered_model_name='bert-encoder'\n",
    "    )\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "train-register-hugging-face-model-serving",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
